### Model 1
For a digital image using RGB color mode, we first separate the three color channels of red, green and blue.The image data of each channel can be represented by a 2-dimensional matrix. Each element in the 2-dimensional matrix corresponds to one pixel, and its value is the pixel's gray value. After obtaining the three 2-dimensional matrices, We first convert it from RGB to YCbCr. Where Y is the brightness component, Cb is the blue color component, and Cr is the red color component.The human eye is more sensitive to the Y component, so it will be less noticeable to the naked eye when subsampling the chromaticity component to reduce the chromaticity component.YCbCr color space and RGB color space conversion formulas are as follows:
\[\begin{cases}Y=0.299R+0.587G+0.114B\\Cb=0.564(B-Y)\\Cr=0.713(R-Y)\end{cases}\]
\[\begin{cases}R=Y+1.402Cr\\G=Y-0.344Cb-0.714Cr\\B=Y+1.772Cb\end{cases}\]

We decide use YCbCr 4:2:0 format,which is by far the most common color representation used in compressedimages and video.[1] 


This sampling is to reach a certain lossy compression without affecting the visual experience.
We divide them into sub-blocks separately to improve the efficiency of subsequent operations. Our specific operation is to divide the original matrix into sub-blocks of 8 × 8 in order from left to right and from top to bottom. After this step, A sub-block is considered as the basic unit to operate.
There are 2 things to note about blocking.
* Each value in the 2-dimensional matrix needs to be subtracted by 128 before blocking so that the range of each value in the 2-dimensional matrix is changed from 0 to 255 to -128 to 127.
* The length or width of the original image is not a multiple of 8, which needs to make up to be multiples of 8 so that the 2-dimensional matrix is completely divided into several sub-blocks.
There are 3advantages to blocking:
* To facilitate the DCT.
* There are no restrictions on the length-width ratio of the image.
* Take into account the local similarity of the picture.

The discrete cosine transform (DCT) is a technique for converting a signal into elementary frequency components. It is widely used in image compression[2]. And our model uses 2-dimensional DCT.The most common DCT definition of a 1-D sequence of length N is 
\[C(u)=\alpha(u)\sum_{x=0}^{N-1}f(x)\cos\left[\frac{(2x+1)u\pi}{2N}\right]\]
for $u$ $=$ 0,1,2...,$N-1$. The inverse transformation is defined as
\[f(x)=\sum_{x=0}^{N-1}\alpha(u)C(u)\cos\left[\frac{(2x+1)u\pi}{2N}\right]\]
for $u$ $=$ 0,1,2...,$N-1$. In both equations (?) and (?) $\alpha(u)$ is defined as 
\[\alpha(u)=\begin{cases}\sqrt{\frac1N},&\text{for }u=0\\
   \sqrt{\frac2N},&\text{for } u\not= 0
   \end{cases}
\]
The 2-D DCT is a direct extension of the 1-D case and is given by
\[C(u,v) = \alpha(u)\alpha(v)\sum_{x=0}^{N-1}\sum_{y=0}^{N-1}f(x,y)\cos\left[\frac{(2x+1)u\pi}{2N}\right]\cos\left[\frac{(2y+1)v\pi}{2N}\right]\]
for $u$ $=$ 0,1,2...,$N-1$ and $\alpha(u)$ and $\alpha(v)$ are defined in (?). The inverse transform is defined as 
\[f(x,y) = \sum_{x=0}^{N-1}\sum_{y=0}^{N-1}\alpha(u)\alpha(v)C(u,v)\cos\left[\frac{(2x+1)u\pi}{2N}\right]\cos\left[\frac{(2y+1)v\pi}{2N}\right]\]
for $u$ $=$ 0,1,2...,$N-1$. The 2-D basis functions can be generated by multiplying the horizontally oriented 1-D basis functions with vertically oriented set of the same functions.[?] DCT is the transform of signal from time domain to frequency domain, and its transformation result is not complex, all is real.Each 8\*8 original pixels is changed into 8*8 digits, and each number is composed of original 64 data through the basis function.
The next step is quantization. Quantization here refers to the quantization of the frequency coefficient after FDCT. The aim is to reduce the magnitude of the non-zero coefficient and increase the number of zero coefficients.
[量化表]
The quantization that we do is to divide the pixel value by the quantized table.Because the value in the upper left corner of the quantization table is smaller, the value in the upper right corner is larger, the purpose of maintaining low frequency components and suppressing high frequency components is achieved.
After that, Zig-Zag scan is performed on the quantized change coefficient. The so-called Zig-Zag scan is to start from the upper left corner of the matrix and scan according to the shape of the letter 'Z'. The purpose of the scanning is as follows:
1. to group low frequency coefficients in top of vector. 
2. Maps m x n to a 1 x m*n vector.

[图]
After Zig-Zag scanning, the low-frequency components in this one-dimensional vector are in the front, and the high frequency components are later. It is worth noting that the high frequency component has a large number of continuous zero values, which is making for 0-RLE.
0-RLE is an encoded mode we create which referred to RLE and aimming at zero. RLE(Run-length encoding) is a very simple form of lossless data compression in which runs of data (that is, sequences in which the same data value occurs in many consecutive data elements) are stored as a single data value and count, rather than as the original run. This is most useful on data that contains many such runs. For example, With a RLE data compression algorithm applied to the a character string 'AAAABBB', it can be rendered as follows：’4A3B’. 0-RLE only do special coding for 0, other values are reserved. The specific step is as follows:
Initialize a queue, and the encoder will scan the elements of the vector to the tail. If zero is encountered, the num of zeroes is counted until a non-zero is encountered. If the non-zero value is encountered, zero and its count are pressed into the queue. If the next number is non-zero, the current number is pressed into the queue. When the tail is read, if the value is zero, zero and its count are pressed into the queue; if the value is non-zero, it is pressed into the queue.For example, With a RLE data compression algorithm applied to the a digital string '0 0 1 2 3 0 1 0 0 0 0 0 0 0 1', it can be rendered as follows :'0 2 1 2 3 0 1 1 0 7 1'.
Final, the 1-dimensional vector obtained from last step is encoded with huffman coding.Huffman coding is the basic method of image data scan again and calculate the probability of all pixels appear, according to the size of the probability of different length of code word only, the greater the probability of the pixels shorter code. The result of huffman coding we do is a hoffman collation of the image. The encoded data is the code word corresponding to each pixel, and the corresponding relation between the code word and the actual pixel value is recorded in the code table.
For a set of symbols with a uniform probability distribution and a number of members which is a power of two, Huffman coding is equivalent to simple binary block encoding.

Input.
Alphabet  ${\displaystyle A=\left\{a_{1},a_{2},\cdots ,a_{n}\right\}}$, which is the symbol alphabet of size ${\displaystyle n}$.
Tuple ${\displaystyle W=(w_{1},w_{2},\cdots ,w_{n})}$, which is the tuple of the (positive) symbol weights (usually proportional to probabilities), i.e. ${\displaystyle w_{i}=\mathrm {weight} \left(a_{i}\right),1\leq i\leq n}$.

Output.
Code ${\displaystyle C\left(W\right)=(c_{1},c_{2},\cdots ,c_{n})}$, which is the tuple of (binary) codewords, where$ {\displaystyle c_{i}}$ is the codeword for ${\displaystyle a_{i},1\leq i\leq n}$.

Goal.
Let ${\displaystyle L\left(C\left(W\right)\right)=\sum _{i=1}^{n}{w_{i}\times \mathrm {length} \left(c_{i}\right)}} {\displaystyle L\left(C\left(W\right)\right)=\sum _{i=1}^{n}{w_{i}\times \mathrm {length} \left(c_{i}\right)}} $be the weighted path length of code ${\displaystyle C}$ C. Condition: ${\displaystyle L\left(C\left(W\right)\right)\leq L\left(T\left(W\right)\right)}$ ${\displaystyle L\left(C\left(W\right)\right)\leq L\left(T\left(W\right)\right)} $for any code ${\displaystyle T\left(W\right)}$
[图片]
For the above steps, the steps before the Zig-Zag scan is a lossy image compression to eliminate data redundancy, which is unble to recover after inverse transform. the steps after the Zig-Zag scan is a lossless image compression to eliminate encoding redundancy, which is able to reduce storage space needed.
